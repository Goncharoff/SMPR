# Метрические алгоритмы классификации
 Алгоритм k ближайших соседей - kNN относит объект u к тому классу элементов которого больше среди k ближайших соседей 
![equation](http://latex.codecogs.com/gif.latex?x_u^{i},&space;i=1,...,k:)

![equation](http://latex.codecogs.com/gif.latex?w(i,&space;u)&space;=&space;[i&space;\leq&space;k];&space;a(u;&space;X^l,&space;k)&space;=&space;argmax_{y\epsilon&space;Y}&space;\sum^k_{i&space;=&space;1}{[y^i_{u}&space;=&space;y]})

Для выбора оптимального k используют метод скользащего контроля (LOO).
Применив kNN и LOO к датасету Ириса Фишера получим результат:

![](https://github.com/Goncharoff/SMPR/blob/master/imgs/LOO_kNN.png)

Видно, что лучишй результат получаем при k = 6, с оценкой ошибки равной 0.33, что равно 96% успешных классификаций.

# Линейные классификаторы
### Adaline
  Алгоритм классификации ADALINE— адаптивны линейный элемент, в качестве функции потерь используется квадратичная функция потерь:
![equation](http://latex.codecogs.com/gif.latex?(<w,x>&space;-&space;y_i)^2)
 Алгоритм обучается с помошью стохастического градиента.

  Обучение ADALINE заключается в подборе "наилучших" значений вектора весов w. Какие значение весов лучше определяет функционал потерь. В ADALINE используется функционал, предложенный Видроу и Хоффом, ![equation](http://latex.codecogs.com/gif.latex?L(a,x)&space;=&space;(a-y)^2). Таким образом необходимо минимизировать функционал ![equation](http://latex.codecogs.com/gif.latex?L(a,x)&space;=&space;Q(w)): 

![equation](http://latex.codecogs.com/gif.latex?$$Q(w)=&space;\sum^m_{i=1}{(a(x_i,w)-y_i)^2}\rightarrow&space;min_w$$).
Построим алгоритм на классификации ирисов Фишера, с классами virginica и versicolo по 3 и 4 параметрам datafram'а. Проверим 
работу алгоритма на 2,5,10 и 43 шагах стохастического градиента. Получим: 
![](https://github.com/Goncharoff/SMPR/blob/master/imgs/adadline_result.png)

Линия на графике - разделяющая гиперплоскость. Оптимум достигается на 43'ом шаге.
Веса получились равны: w_1 = 3.24, w_2 = -1.7

#### Перцептрон Розенблата
Будем ипсользовать virginica и versicolor. По 3 и 4 параметрам datafram'а

Будем проверять результаты работы на 10, 75, 230, 275 шаге стохастического градиентного спуска. В реузльтате получим:

![](https://github.com/Goncharoff/SMPR/blob/master/perceptron.png)

Линия на графике - раздиляющая гиперплоскость. Оптимум достигается при 275'ом шаге.
В результате получили веса, равные w_1 = -102.9, w_2 = 152.1, w_3 = 119.


### Логическая регрессия
Логическая регрессия - линейный байесовский классификатор, использующий логарифмическую функцию потерь: 
![equation](http://latex.codecogs.com/gif.latex?L(M)&space;=&space;\log_2(1&plus;e^{-M}))
Метод логистической регрессии основан на довольно сильных вероятностных
предположениях, которые имеют несколько интересных последствий:
1. Линейный классификатор оказывается оптимальным байесовским;
2. Однозначно определяется функция потерь;
3. Можно вычислять не только принадлежность объектов классам, но
также получать и численные оценки вероятности их принадлежности.
  Обучения происходит с помошью стохастического градиента.
  Необходимо мниимизировать функционал ![equation](http://latex.codecogs.com/gif.latex?Q(w,&space;X^l)&space;=&space;\sum_{i=1}^l&space;{\log_2&space;\sigma(<w,&space;x_i>y_i)&space;&plus;&space;const(w)}&space;\rightarrow&space;max_w)

  Построим алгоритм на классификации ирисов Фишера, с классами virginica и versicolo по 3 и 4 параметрам datafram'а. Проверим 
работу алгоритма на 2,5,10 и 8791 шагах стохастического градиента. Получим: 
![](https://github.com/Goncharoff/SMPR/blob/master/imgs/log_reg_result.png)

Линия на графике - разделяющая гиперплоскость. Оптимум достигается на 8791'ом шаге.
Веса получились равны: w_1 = 0.56, w_2 = -1.54

Достоинства логистической регрессии:

• Как правило, логистическая регрессия дает лучшие результаты по сравнению с линейным дискриминантом Фишера (поскольку она основана на менее жестких гипотезах), а также по сравнению с дельта-правилом и правилом Хэбба (поскольку она использует „более правильную” функцию потерь).

•Возможность оценивать апостериорные вероятности и риски.

Недостатки логистической регрессии:

•Оценки вероятностей и рисков могут оказаться неадекватными, если не выполняются предположения теоремы.

•Градиентный метод обучения логистической регрессии наследует все недостатки метода стохастического градиента. Практичная реализация должна предусматривать стандартизацию данных, отсев выбросов, регуляризацию (сокращение весов), отбор признаков, и другие эвристики для улучшения сходимости. Возможно применение метода второго порядка, но он требует обращения n×n-матриц на каждом шаге и nакже не застрахован от плохой сходимости.

### Итоговый график для трех классификаторов:

![](https://github.com/Goncharoff/SMPR/blob/master/imgs/all_results.png)


### SVM
Метод опорных векторов

В настоящее время метод опорных векторов (SVM) считается одним из лучших методов классификации!
Метод SVM обладает несколькими замечательными свойствами:

1. Обучение SVM сводится к задаче квадратичного программирования, имеющей единственное решение, которое вычисляется достаточно эффективно даже на выборках в сотни тысяч объектов;
2. Решение обладает свойством разреженности: положение оптимальной разделяющей гиперплоскости зависит лишь от небольшой доли обучающих объектов. Они и называются опорными векторами, остальные объекты фактически не задействуются;
СМПР
3. С помощью функции ядра метод обобщается на случай нелинейных разделяющих поверхностей. Вопрос о выборе ядра, оптимального для
данной прикладной задачи, до сих пор остается открытой теоретической проблемой.

Будем ипсользовать virginica и versicolor. По 3 и 4 параметрам datafram'а

В результате получим: 
![](https://github.com/Goncharoff/SMPR/blob/master/imgs/SVM_resul.png)
Благодоря библиотеке kernlab выведем также полученный результат:
Support Vector Machine object of class "ksvm" 

SV type: C-svc  (classification) 
 parameter : cost C = 1 

Gaussian Radial Basis kernel function. 
 Hyperparameter : sigma =  2.22977846013437 

Number of Support Vectors : 30 

Objective Function Value : -8.5529 
Training error : 0 
